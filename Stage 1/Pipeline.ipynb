{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.read_pickle('merged.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "m_size = 10000\n",
    "\n",
    "# Stratified Test Train Split\n",
    "test_size = 0.2\n",
    "\n",
    "# Doc2Vec hyperparameters\n",
    "vector_size = 400\n",
    "min_count = 2\n",
    "epochs = 20\n",
    "window = 10\n",
    "\n",
    "#Embedding Inferrence\n",
    "steps=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleData(df, m_size):\n",
    "    \"\"\"\n",
    "    Sample the data with hyperparameter m_size specifying how many training examples we want\n",
    "    Hyperparameters: m_size\n",
    "    \n",
    "    :param df: Input Pandas DataFrame\n",
    "    :type df: pd.DataFrame\n",
    "    :param m_size: Number of training examples desired\n",
    "    :type m_size: int\n",
    "    \n",
    "    :return: X, y of combined sampled training data and labels for the model\n",
    "    :rtype: List*2\n",
    "    \"\"\"\n",
    "    \n",
    "    pos = df[df['label'] == 1]\n",
    "    neg = df[df['label'] == 0].sample(m_size, random_state=42)\n",
    "    data = neg.append(pos)\n",
    "    combined = [(h + ' ' + s + ' ' + b, l) for h, s, b, l in \n",
    "                    zip(list(data['headline_x']), list(data['summary_x']), list(data['body_x']), list(data['label']))]\n",
    "    print('Sampling Done')\n",
    "    return zip(*combined)\n",
    "\n",
    "\n",
    "\n",
    "def stratSpl(X, y, test_size):\n",
    "    \"\"\"\n",
    "    Make a stratified test/train split to use for training and testing.\n",
    "    Hyperparameters: None\n",
    "    \n",
    "    :param X: Input features of the combined (train and test) sampled set. \n",
    "    :type X: List\n",
    "    :param y: Input labels of the combined (train and test) sampled set\n",
    "    :type y: List\n",
    "    :param test_size: Test ratio to split up. Number between 0 and 1\n",
    "    :type test_size: Float\n",
    "    \n",
    "    :return: 4 Lists corresponding to X_tr, X_te, y_tr, y_te\n",
    "    :rtype: List*4\n",
    "    \"\"\"\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_tr, X_te = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "        y_tr, y_te = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "    \n",
    "    print('Stratified test/train split done')\n",
    "    return X_tr, y_tr, X_te, y_te\n",
    "\n",
    "\n",
    "\n",
    "def prepare(X_tr, X_te):\n",
    "    \"\"\"\n",
    "    Prepare the data (using gensims simple_preprocess)\n",
    "    Hyperparameters: None\n",
    "    \n",
    "    :param X_tr: Training features\n",
    "    :type X_tr: List\n",
    "    :param X_te: Testing features\n",
    "    :type X_te: List\n",
    "    \n",
    "    :return: Processed version of X_tr and X_te.\n",
    "    :rtype: List*2\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def read_corpus(data):\n",
    "        for i, line in enumerate(data):\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), tags=[i])\n",
    "    \n",
    "    train_corpus = list(read_corpus(X_tr))\n",
    "    test_corpus = list(read_corpus(X_te))\n",
    "    \n",
    "    print('Data preparation done')\n",
    "    \n",
    "    return train_corpus, test_corpus\n",
    "\n",
    "\n",
    "def doc2vec_model_train(X_tr, vector_size, min_count, epochs, window):\n",
    "    \"\"\"\n",
    "    Doc2Vec model defined and trained via this function. \n",
    "    Hyperparameters: size, min_count, epochs, window\n",
    "    \n",
    "    :param X_tr: Training data\n",
    "    :type X_tr: List\n",
    "    :param vector_size: Dimensionality of the feature vectors \n",
    "    :type vector_size: Int\n",
    "    :param min_count: Minimum occurences for which to still keep a word in the vocab.\n",
    "    :type min_count: Int\n",
    "    :param epochs: Number of epochs for which the model trains\n",
    "    :type epochs: Int\n",
    "    :param window: Window size of context to consider in a given instance.\n",
    "    :type window: Int\n",
    "    \n",
    "    :return: Fully trained model\n",
    "    :rtype: gensim model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=vector_size, min_count=min_count, window=window, epochs=epochs)\n",
    "    model.build_vocab(X_tr)\n",
    "    model.train(X_tr, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    print('Doc2Vec Model Trained')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def embeddings(model, X, steps):\n",
    "    \n",
    "    \"\"\"\n",
    "    Embed documents into vector space for classification in the next stage.\n",
    "    Hyperparameters: steps\n",
    "    \n",
    "    :param model: Trained Doc2Vec model\n",
    "    :type model: gensim Doc2Vec model\n",
    "    :param X: Input corpus\n",
    "    :type X: List of TaggedDocuments\n",
    "    :param steps: Hyperparameter to tune\n",
    "    :type steps: Int\n",
    "    \n",
    "    :return: Embedded feature vector\n",
    "    :rtype: List\n",
    "    \"\"\"\n",
    "    \n",
    "    z = [model.infer_vector(X[doc_id].words, steps=steps) for doc_id in range(len(X))]\n",
    "    \n",
    "    print('Documents embedded into vector space')\n",
    "    \n",
    "    return z\n",
    "\n",
    "\n",
    "def FinalClassifier(X_tr, y_tr):\n",
    "    \n",
    "    \"\"\"\n",
    "    Models for final classifcation, will be hyperparameters\n",
    "    Hyperparameters: The models themselves and their hyperparameters *Come back here for alteration\n",
    "    \n",
    "    :param X_tr: Input document vectors \n",
    "    :type X_tr: List\n",
    "    :param y_tr: Labels\n",
    "    :type: List\n",
    "    \n",
    "    :return: Trained logreg model\n",
    "    :rtype: \n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(random_state=42).fit(X_tr, y_tr)\n",
    "    print('Final Classifier Trained.')\n",
    "    \n",
    "    return clf\n",
    "    \n",
    "\n",
    "def precision(conf):\n",
    "    num = conf[0][1]\n",
    "    den = num + conf[1][1]\n",
    "\n",
    "    return num/den\n",
    "\n",
    "def recall(conf):\n",
    "    num = conf[1][1]\n",
    "    den = num + conf[1][0]\n",
    "    \n",
    "    return num/den\n",
    "\n",
    "def F1(P, R):\n",
    "    return 2 * P*R/(P+R)\n",
    "\n",
    "def average(l):\n",
    "    return functools.reduce(lambda x, y: x + y, l) / len(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sampleData(merged, m_size)\n",
    "\n",
    "X_tr, y_tr, X_te, y_te = stratSpl(X, y, test_size)\n",
    "\n",
    "X_tr, X_te = prepare(X_tr, X_te)\n",
    "\n",
    "d2v = doc2vec_model_train(X_tr, vector_size, min_count, epochs, window)\n",
    "\n",
    "X_tr = embeddings(d2v, X_tr, steps)\n",
    "\n",
    "clf = FinalClassifiers(X_tr, y_tr)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te = embeddings(d2v, X_te, steps)\n",
    "\n",
    "y_pr = clf.predict(X_te)\n",
    "\n",
    "y_sc = clf.decision_function(X_te)\n",
    "\n",
    "conf = confusion_matrix(y_te, y_pr)\n",
    "\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R = precision(conf), recall(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef1 = F1(P,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = average_precision_score(y_te, y_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, ef1, average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot precision vs recall\n",
    "\n",
    "average_precision = average_precision_score(y_te, y_sc)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_te, y_sc)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
