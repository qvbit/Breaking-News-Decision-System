{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy as spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import collections\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# models to try\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_MAP = {'ADJ' : 0,\n",
    "          'ADP' : 1,\n",
    "          'ADV': 2,\n",
    "          'AUX': 3,\n",
    "          'CONJ': 4,\n",
    "          'CCONJ': 5,\n",
    "          'DET': 6,\n",
    "          'INTJ': 7,\n",
    "          'NOUN': 8,\n",
    "          'NUM': 9,\n",
    "          'PART': 10,\n",
    "          'PRON': 11,\n",
    "          'PROPN': 12,\n",
    "          'PUNCT': 13,\n",
    "          'SCONJ': 14,\n",
    "          'SYM': 15,\n",
    "          'VERB': 16,\n",
    "          'X': 17,\n",
    "          'SPACE': 18}\n",
    "\n",
    "ENT_MAP = {'PERSON': 0,\n",
    "          'NORP': 1,\n",
    "          'FAC': 2,\n",
    "          'ORG': 3,\n",
    "          'GPE': 4,\n",
    "          'LOC': 5,\n",
    "          'PRODUCT': 6,\n",
    "          'EVENT': 7,\n",
    "          'WORK_OF_ART': 8,\n",
    "          'LAW': 9,\n",
    "          'LANGUAGE': 10,\n",
    "          'DATE': 11,\n",
    "          'TIME': 12,\n",
    "          'PERCENT': 13,\n",
    "          'MONEY': 14,\n",
    "          'QUANTITY': 15,\n",
    "          'ORDINAL': 16,\n",
    "          'CARDINAL': 17}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq_only(df):\n",
    "    df = df[df['label'] == 1]\n",
    "    df.dropna(subset=['body', 'headline', 'summary'], thresh=3, inplace=True)\n",
    "    \n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main starts here\n",
    "df = pd.read_pickle('../dataframes/df_eq_label.pkl')\n",
    "df = eq_only(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['_id', 'body', 'headline', 'summary', 'categories', 'T0', 'T1', 'T2']]\n",
    "df['categories'] = df['categories'].apply(lambda x: '. '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = [t + '. ' + h + '. ' + s + ' ' + b  for t, h, s, b in\n",
    "                        zip(list(df['categories']), list(df['headline']), list(\n",
    "    df['summary']), list(df['body']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_locs = df[['T0', 'T1', 'T2']].fillna(method='bfill', axis='columns')['T0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "docs = [(_id, nlp(doc), t) for doc, _id, t in list(zip(combined, list(df['_id']), target_locs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_features(docs, save=True):\n",
    "    \"\"\"\n",
    "    Returns _id, X, y as three nparrays.\n",
    "    \"\"\"\n",
    "\n",
    "    features = []\n",
    "\n",
    "    num_ents = len(ENT_MAP)\n",
    "    num_pos = len(POS_MAP)\n",
    "    \n",
    "    for _id, doc, t in docs:\n",
    "\n",
    "        names = [ent.text for ent in doc.ents if ent.label_ == 'GPE' or ent.label_=='LOC']\n",
    "        loc_ents = [ent for ent in doc.ents if ent.label_=='GPE' or ent.label_=='LOC']\n",
    "        num_examples = len(names)\n",
    "        doc_num = [_id for _ in range(num_examples)]\n",
    "        target = [t for _ in range(num_examples)]\n",
    "\n",
    "        # Feature 1: Sentence vector in which the ent appears in.\n",
    "        f1 = np.array([ent.sent.vector for ent in doc.ents if ent.label_ == 'GPE' or ent.label_=='LOC'])\n",
    "\n",
    "        # Feature 2: Check surrounidng entity types\n",
    "        f2 = np.zeros((num_examples, num_ents))\n",
    "\n",
    "        for i, e1 in enumerate(loc_ents):\n",
    "            for e2 in e1.sent.ents:\n",
    "                f2[i, ENT_MAP[e2.label_]] = 1\n",
    "\n",
    "        # Feature 3: Check surrounding part of speech tags\n",
    "        f3 = np.zeros((num_examples, num_pos))\n",
    "\n",
    "        for i, e1 in enumerate(loc_ents):\n",
    "            for e2 in e1.sent:\n",
    "                f3[i, POS_MAP[e2.pos_]] = 1\n",
    "\n",
    "        # Feature 4: Token offset for each ent.\n",
    "        f4 = np.array([e.start for e in loc_ents]).reshape(-1, 1)\n",
    "\n",
    "        # Feature 5: How many times that particular ent appears in the entire document.\n",
    "        loc_counts = Counter(names)\n",
    "        f5 = np.array([loc_counts[e.text] for e in loc_ents]).reshape(-1, 1)\n",
    "        \n",
    "        # Feature 6: The word vectors themselves.\n",
    "        f6 = ([ent.vector for ent in doc.ents if ent.label_ =='GPE' or ent.label_=='LOC'])\n",
    "\n",
    "        feature = np.hstack((np.array(doc_num).reshape(-1, 1), np.array(names).reshape(-1, 1), f1, f2, f3, f4, f5, f6, np.array(target).reshape(-1, 1)))\n",
    "        features.append(feature)\n",
    "        \n",
    "        \n",
    "    \n",
    "    num_features = feature.shape[1]\n",
    "    ret = []\n",
    "    \n",
    "    for f in features:\n",
    "        for r in f:\n",
    "            ret.append(r)\n",
    "            \n",
    "    ret = np.array(ret).reshape(-1, num_features)\n",
    "\n",
    "    _id = ret[:, 0]\n",
    "    name = ret[:, 1]\n",
    "    X = ret[:, 2:-1]\n",
    "    y = ret[:, -1] \n",
    "    \n",
    "    y = list(map(lambda x, y: int(x.lower() in y.lower()), y, name))  \n",
    "\n",
    "    return _id, names, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REturn to main\n",
    "\n",
    "_ids, names, X, y = location_features(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2781, 639)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8176904795732535"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Machine learning pipeline\n",
    "\n",
    "scale = StandardScaler()\n",
    "decomp = PCA(n_components=500)\n",
    "svm = SVC()\n",
    "params = {}\n",
    "\n",
    "pipe = Pipeline([('sc', scale), ('pca', decomp), ('clf', svm)])\n",
    "search = GridSearchCV(pipe, params, iid=False, cv=5)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pca__n_components': 500}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8132183908045977"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
